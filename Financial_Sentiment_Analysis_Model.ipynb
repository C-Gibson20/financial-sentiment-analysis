{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_iKNQbnwDvFp"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5QJS/2d4T4LLjuwlMAtsc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C-Gibson20/financial-sentiment-analysis/blob/main/Financial_Sentiment_Analysis_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports..."
      ],
      "metadata": {
        "id": "brN5-4hsa5sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate nlpaug"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fS42yN7ULES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset as HFDataset\n",
        "from evaluate import load\n",
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainerCallback,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup)"
      ],
      "metadata": {
        "id": "m4wJzoNKFPgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation\n"
      ],
      "metadata": {
        "id": "9nCA9btjbKTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaggle Dataset Download"
      ],
      "metadata": {
        "id": "_iKNQbnwDvFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yjS8tCNNBedq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005550b4-1d20-43f7-b041-d4c3a6a73ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "SmGDiBe8S26A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/APIs/kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "gWir_avfS3f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "8OplJEDxS5JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download sbhatti/financial-sentiment-analysis"
      ],
      "metadata": {
        "id": "lcvdVVqwS8Bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5100fb7-6bcb-474d-a952-a021b7d8c123"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis\n",
            "License(s): CC0-1.0\n",
            "Downloading financial-sentiment-analysis.zip to /content\n",
            "100% 276k/276k [00:00<00:00, 559kB/s]\n",
            "100% 276k/276k [00:00<00:00, 559kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip financial-sentiment-analysis.zip"
      ],
      "metadata": {
        "id": "iLkDvR5TS9sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85264c6e-8734-4015-deca-159355865976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  financial-sentiment-analysis.zip\n",
            "  inflating: data.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "B9mV42K4D4bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "tB9xsozQQ9XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(sentences, sentiments, tokenizer):\n",
        "    encodings = tokenizer(sentences, truncation=True, padding=True, return_tensors='pt')\n",
        "    labels = torch.tensor(sentiments)\n",
        "    encodings['labels'] = labels\n",
        "    return encodings\n",
        "\n",
        "def data_loader(encodings, labels, batch_size=32, shuffle=False):\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def augment_sentences(sentences, sentiments, augment_rate=0.2):\n",
        "    aug_sentences, aug_sentiments = [], []\n",
        "    for sentence, sentiment in zip(sentences, sentiments):\n",
        "        aug_sentences.append(sentence)\n",
        "        aug_sentiments.append(sentiment)\n",
        "\n",
        "        if random.random() < augment_rate:\n",
        "            aug_sentence = synonym_aug.augment(sentence)\n",
        "\n",
        "            if isinstance(aug_sentence, list):\n",
        "                aug_sentence = ' '.join(aug_sentence)\n",
        "\n",
        "            aug_sentences.append(aug_sentence)\n",
        "            aug_sentiments.append(sentiment)\n",
        "\n",
        "    return aug_sentences, aug_sentiments"
      ],
      "metadata": {
        "id": "_oAGldpfTRnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_encoding = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "financial_sentiment_df = pd.read_csv('data.csv')\n",
        "financial_sentiment_df['Sentiment'] = financial_sentiment_df['Sentiment'].map(sentiment_encoding)\n",
        "\n",
        "sentences = financial_sentiment_df['Sentence'].tolist()\n",
        "sentiments = financial_sentiment_df['Sentiment'].tolist()\n",
        "\n",
        "train_sentences, val_test_sentences, train_sentiments, val_test_sentiments = train_test_split(sentences, sentiments, test_size=0.2, random_state=42, stratify=sentiments)\n",
        "val_sentences, test_sentences, val_sentiments, test_sentiments = train_test_split(val_test_sentences, val_test_sentiments, test_size=0.5, random_state=42, stratify=val_test_sentiments)\n",
        "\n",
        "synonym_aug = naw.SynonymAug(aug_src='wordnet', aug_max=2)\n",
        "aug_train_sentences, aug_train_sentiments = augment_sentences(train_sentences, train_sentiments)"
      ],
      "metadata": {
        "id": "gQzMSBSEOHbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd49464-428b-42a8-d8d9-1fa45f00d5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = encode(aug_train_sentences, aug_train_sentiments, tokenizer)\n",
        "val_encodings = encode(val_sentences, val_sentiments, tokenizer)\n",
        "test_encodings = encode(test_sentences, test_sentiments, tokenizer)\n",
        "\n",
        "train_loader = data_loader(train_encodings, train_encodings['labels'], shuffle=True)\n",
        "val_loader = data_loader(val_encodings, val_encodings['labels'])\n",
        "test_loader = data_loader(test_encodings, test_encodings['labels'])"
      ],
      "metadata": {
        "id": "C442q1yF7FTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_sentiments), y=train_sentiments)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')"
      ],
      "metadata": {
        "id": "wuTbC3Wqqls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training and Initialisation"
      ],
      "metadata": {
        "id": "K1IUJDWbb1-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)"
      ],
      "metadata": {
        "id": "4jOYUpSY3TXc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "d00b58bf825045618ef8b7287917512a",
            "188a6dbae8fb40248088e7f3581fe827",
            "c38cf6ef376b4fa7b8c3186ecf4a38fc",
            "664ba098b4594fffbda7bd152137da2b",
            "9d5640f54536498eb4b2ae9d8a61b390",
            "2f3b720db209443ca36c1202c7b4a455",
            "01f33fdee11640dfb09973da3a9e498c",
            "5c34b1faa5434010819386f53dc34f63",
            "d224d0bb091147caad6feaeb829a3a19",
            "382db521e9dd4f2c9576988fc3384a9f",
            "1e60c97c0c35440ab62db4fd5900088b"
          ]
        },
        "outputId": "6b59a82b-5476-4592-b2e7-b6865d8ee6d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimizer_with_warmup(model, num_warmup_steps=500, total_steps=5000, learning_rate=2e-5, dft_rate=1.2):\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = []\n",
        "\n",
        "    for i in range(12):\n",
        "        layer_decay = {\n",
        "            'params': [p for n, p in model.bert.encoder.layer[i].named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.01,\n",
        "            'lr': learning_rate / (dft_rate ** (11 - i))\n",
        "        }\n",
        "        layer_no_decay = {\n",
        "            'params': [p for n, p in model.bert.encoder.layer[i].named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            'weight_decay': 0.0,\n",
        "            'lr': learning_rate / (dft_rate ** (11 - i))\n",
        "        }\n",
        "        optimizer_grouped_parameters.append(layer_decay)\n",
        "        optimizer_grouped_parameters.append(layer_no_decay)\n",
        "\n",
        "    classifier_params = {\n",
        "        'params': model.classifier.parameters(),\n",
        "        'lr': learning_rate,\n",
        "        'weight_decay': 0.01\n",
        "    }\n",
        "\n",
        "    optimizer_grouped_parameters.append(classifier_params)\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "    return optimizer, scheduler"
      ],
      "metadata": {
        "id": "5aQeIdTsrAcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradualUnfreezingCallback(TrainerCallback):\n",
        "    def __init__(self, model, freeze_start_layer=11):\n",
        "        self.model = model\n",
        "        self.currently_frozen_layer = freeze_start_layer\n",
        "\n",
        "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
        "        if self.currently_frozen_layer >= 0:\n",
        "            for param in self.model.bert.encoder.layer[self.currently_frozen_layer].parameters():\n",
        "                param.requires_grad = True\n",
        "            print(f\"Unfreezing layer {self.currently_frozen_layer}\")\n",
        "            self.currently_frozen_layer -= 1\n",
        "\n",
        "for param in model.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "0oAHz05Rrmn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, class_weights=None):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss(weight=self.class_weights)(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "        return focal_loss"
      ],
      "metadata": {
        "id": "QFGSbUt87YbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, class_weights, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = FocalLoss(class_weights=self.class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "J_F7_irZiLh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    metric_accuracy = load('accuracy')\n",
        "    metric_f1 = load('f1')\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)\n",
        "    f1 = metric_f1.compute(predictions=predictions, references=labels, average='weighted')\n",
        "    return {'accuracy': accuracy['accuracy'], 'f1': f1['f1']}"
      ],
      "metadata": {
        "id": "1q1uF6jeNJvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer, scheduler = create_optimizer_with_warmup(model)\n",
        "gradual_unfreezing_callback = GradualUnfreezingCallback(model)\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.01)\n",
        "\n",
        "train_dataset = HFDataset.from_dict(train_encodings)\n",
        "val_dataset = HFDataset.from_dict(val_encodings)\n",
        "test_dataset = HFDataset.from_dict(test_encodings)"
      ],
      "metadata": {
        "id": "NjuMkiVcNCbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.001,\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    greater_is_better=True,\n",
        "    warmup_steps=500,\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    optimizers=(optimizer, scheduler),\n",
        "    callbacks=[gradual_unfreezing_callback, early_stopping_callback],\n",
        "    class_weights=class_weights\n",
        ")"
      ],
      "metadata": {
        "id": "U7cUUlVs7sTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "f7133b382af64642a726e91471bfd041",
            "d369684e750548abb5906cc1fd5bd2f5",
            "19508e6cc2b1418d97b59f4aa1324a1a",
            "4e8bd053569f4a10b87d3d52cabda920",
            "935531653d2247d2ae8de480d820abcf",
            "8ea5fc5e51dc47cbbf8dbed3671802b9",
            "4e6fe866601c4a1bb902f1ad8adb1278",
            "968dc85cd76a409986e4d02d05b6295a",
            "ca0b5d8fba9b4c0b84a38183eb08209f",
            "033d778df98f45be856fc06f57a87565",
            "be941ce94aa84252b9855c6ec32a5fed",
            "886e042c39674669856a67285b286a05",
            "3178f332f0884e468519b5ffa749f06f",
            "fa9511b88c5f41e6a409b7506ffbb96e",
            "63020a63f9584f689d98ef0707782e2d",
            "7ffa783550ed408baa34e1a2709141b6",
            "29878d6b7790437ebdd49c3c27f09246",
            "928861da8b6a46e09853a8dd795085a7",
            "a398acfbac0548ec84767a88a5552f61",
            "11be729eab8144e28fa02048494234ce",
            "bbad07f6a9de4f009cfd399b3b904728",
            "893e92af4718470b8c101a32de473e95"
          ]
        },
        "id": "vIuFSG28NVkV",
        "outputId": "e0c94763-ce47-45a7-cbd8-4c608e71ec89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfreezing layer 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1239' max='1770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1239/1770 07:39 < 03:17, 2.69 it/s, Epoch 7/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.458430</td>\n",
              "      <td>0.583904</td>\n",
              "      <td>0.572282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.253648</td>\n",
              "      <td>0.635274</td>\n",
              "      <td>0.648155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>0.135426</td>\n",
              "      <td>0.720890</td>\n",
              "      <td>0.739324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>0.116202</td>\n",
              "      <td>0.746575</td>\n",
              "      <td>0.761829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.355000</td>\n",
              "      <td>0.124441</td>\n",
              "      <td>0.756849</td>\n",
              "      <td>0.770158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.069800</td>\n",
              "      <td>0.127756</td>\n",
              "      <td>0.758562</td>\n",
              "      <td>0.772911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.069800</td>\n",
              "      <td>0.132056</td>\n",
              "      <td>0.756849</td>\n",
              "      <td>0.770886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7133b382af64642a726e91471bfd041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "886e042c39674669856a67285b286a05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfreezing layer 10\n",
            "Unfreezing layer 9\n",
            "Unfreezing layer 8\n",
            "Unfreezing layer 7\n",
            "Unfreezing layer 6\n",
            "Unfreezing layer 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1239, training_loss=0.17799699104438393, metrics={'train_runtime': 462.1837, 'train_samples_per_second': 122.159, 'train_steps_per_second': 3.83, 'total_flos': 1990389282331464.0, 'train_loss': 0.17799699104438393, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "hCZaEyImr8SP",
        "outputId": "f51d19d0-88e8-4369-c60a-d40d786fbec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/19 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.08022227138280869,\n",
              " 'eval_accuracy': 0.7965811965811965,\n",
              " 'eval_f1': 0.8099998432077777,\n",
              " 'eval_runtime': 6.0054,\n",
              " 'eval_samples_per_second': 97.413,\n",
              " 'eval_steps_per_second': 3.164,\n",
              " 'epoch': 7.0}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_name = 'finetuned_bert_with_discriminative_ft'\n",
        "model.save_pretrained(saved_model_name)\n",
        "tokenizer.save_pretrained(saved_model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4dvJctBuKOA",
        "outputId": "aeb6297d-c5a1-4d90-9893-a1ba6e95617b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('finetuned_bert_with_discriminative_ft/tokenizer_config.json',\n",
              " 'finetuned_bert_with_discriminative_ft/special_tokens_map.json',\n",
              " 'finetuned_bert_with_discriminative_ft/vocab.txt',\n",
              " 'finetuned_bert_with_discriminative_ft/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0)\n",
        "\n",
        "sentence = \"The market is performing poorly today.\"\n",
        "result = sentiment_pipeline(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8maALr7PuO8U",
        "outputId": "c7f2d742-2142-4693-c9d4-bfff16678937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'LABEL_0', 'score': 0.7712957262992859}]\n"
          ]
        }
      ]
    }
  ]
}
